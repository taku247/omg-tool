#!/usr/bin/env python3
"""
data_preprocessor.py
====================
ä¸è¦å‰‡æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›ãƒ»å‰å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

é–¾å€¤ãƒ™ãƒ¼ã‚¹ã§è¨˜éŒ²ã•ã‚ŒãŸä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’ã€ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨ã«
æ™‚é–“åŒæœŸã•ã‚ŒãŸè¦å‰‡çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚

ä¸»ãªæ©Ÿèƒ½:
- æ™‚é–“çª“ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿é›†ç´„
- æ¬ æãƒ‡ãƒ¼ã‚¿ã®è£œé–“å‡¦ç†
- å–å¼•æ‰€é–“ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿åŒæœŸ
- ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‡ºåŠ›

ä½¿ã„æ–¹:
    python data_preprocessor.py --date 20250623 --window 30s
    python data_preprocessor.py --date 20250623 --window 1min --interpolate
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import argparse
import logging
from typing import Dict, List, Optional, Tuple
import sys

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class DataPreprocessor:
    """ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        self.exchanges = ["bybit", "hyperliquid", "gateio", "kucoin"]
        self.symbols = ["BTC", "ETH", "SOL", "XRP"]
        self.raw_data = {}
        self.processed_data = {}
        
    def load_data(self, date: str) -> Dict[str, pd.DataFrame]:
        """æŒ‡å®šæ—¥ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        logger.info(f"ğŸ“‚ {date}ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        date_dir = self.data_dir / date
        if not date_dir.exists():
            raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {date_dir}")
            
        for exchange in self.exchanges:
            csv_file = date_dir / f"{exchange}_prices_{date}.csv"
            
            if csv_file.exists():
                try:
                    df = pd.read_csv(csv_file)
                    df['timestamp'] = pd.to_datetime(df['timestamp'])
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    df = self._clean_data(df, exchange)
                    
                    if len(df) > 0:
                        self.raw_data[exchange] = df
                        logger.info(f"âœ… {exchange}: {len(df)}è¡Œ èª­ã¿è¾¼ã¿å®Œäº†")
                    else:
                        logger.warning(f"âš ï¸ {exchange}: ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã«ãƒ‡ãƒ¼ã‚¿ãŒç©ºã«ãªã‚Šã¾ã—ãŸ")
                        
                except Exception as e:
                    logger.error(f"âŒ {exchange}: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ - {e}")
            else:
                logger.warning(f"âš ï¸ {exchange}: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                
        logger.info(f"ğŸ“Š èª­ã¿è¾¼ã¿å®Œäº†: {len(self.raw_data)}å–å¼•æ‰€")
        return self.raw_data
        
    def _clean_data(self, df: pd.DataFrame, exchange: str) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        original_len = len(df)
        
        # å¿…é ˆã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèª
        required_cols = ['timestamp', 'symbol', 'bid', 'ask']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            logger.warning(f"âš ï¸ {exchange}: å¿…é ˆã‚«ãƒ©ãƒ ãŒä¸è¶³ - {missing_cols}")
            return pd.DataFrame()
            
        # ç„¡åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»
        mask = (
            df['bid'].notna() & df['ask'].notna() &
            (df['bid'] > 0) & (df['ask'] > 0) &
            (df['bid'] <= df['ask'])  # bid > ask ã®ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»
        )
        
        df_clean = df[mask].copy()
        
        # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å» (åŒä¸€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ»ã‚·ãƒ³ãƒœãƒ«)
        df_clean = df_clean.drop_duplicates(subset=['timestamp', 'symbol'], keep='first')
        
        removed = original_len - len(df_clean)
        if removed > 0:
            logger.info(f"ğŸ§¹ {exchange}: {removed}è¡Œã®ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»")
            
        return df_clean
        
    def create_time_windows(self, window_size: str = "30s", 
                          start_time: Optional[datetime] = None, 
                          end_time: Optional[datetime] = None) -> pd.DatetimeIndex:
        """æ™‚é–“çª“ã®ä½œæˆ"""
        if not self.raw_data:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            
        # å…¨å–å¼•æ‰€ã®æ™‚é–“ç¯„å›²ã‚’å–å¾—
        all_timestamps = []
        for df in self.raw_data.values():
            all_timestamps.extend(df['timestamp'].tolist())
            
        if not all_timestamps:
            raise ValueError("ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
        data_start = start_time or min(all_timestamps)
        data_end = end_time or max(all_timestamps)
        
        # æ™‚é–“çª“ã‚’ä½œæˆ
        time_windows = pd.date_range(
            start=data_start.floor(window_size),
            end=data_end.ceil(window_size),
            freq=window_size
        )
        
        logger.info(f"â° æ™‚é–“çª“ä½œæˆ: {len(time_windows)}å€‹ ({window_size}é–“éš”)")
        logger.info(f"ğŸ“… æœŸé–“: {data_start} ï½ {data_end}")
        
        return time_windows
        
    def aggregate_data(self, window_size: str = "30s", 
                      method: str = "last") -> Dict[str, pd.DataFrame]:
        """æ™‚é–“çª“ã§ã®ãƒ‡ãƒ¼ã‚¿é›†ç´„"""
        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿é›†ç´„ä¸­: {window_size}çª“, {method}æ–¹å¼...")
        
        time_windows = self.create_time_windows(window_size)
        
        for exchange, df in self.raw_data.items():
            aggregated_data = []
            
            for symbol in self.symbols:
                symbol_data = df[df['symbol'] == symbol].copy()
                if len(symbol_data) == 0:
                    continue
                    
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆ
                symbol_data = symbol_data.sort_values('timestamp')
                
                # å„æ™‚é–“çª“ã§ã®ãƒ‡ãƒ¼ã‚¿é›†ç´„
                window_data = []
                for i in range(len(time_windows) - 1):
                    window_start = time_windows[i]
                    window_end = time_windows[i + 1]
                    
                    # è©²å½“æ™‚é–“çª“ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                    mask = (symbol_data['timestamp'] >= window_start) & \
                           (symbol_data['timestamp'] < window_end)
                    window_df = symbol_data[mask]
                    
                    if len(window_df) > 0:
                        # é›†ç´„æ–¹æ³•ã«å¿œã˜ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
                        if method == "last":
                            aggregated = window_df.iloc[-1].copy()
                        elif method == "mean":
                            aggregated = window_df.select_dtypes(include=[np.number]).mean()
                            aggregated['symbol'] = symbol
                            aggregated['exchange'] = exchange
                        elif method == "ohlc":
                            # OHLCå½¢å¼ã§ã®é›†ç´„
                            ohlc_data = {
                                'open_bid': window_df['bid'].iloc[0],
                                'high_bid': window_df['bid'].max(),
                                'low_bid': window_df['bid'].min(),
                                'close_bid': window_df['bid'].iloc[-1],
                                'open_ask': window_df['ask'].iloc[0],
                                'high_ask': window_df['ask'].max(),
                                'low_ask': window_df['ask'].min(),
                                'close_ask': window_df['ask'].iloc[-1],
                                'volume': window_df['volume_24h'].sum() if 'volume_24h' in window_df.columns else 0,
                                'symbol': symbol,
                                'exchange': exchange
                            }
                            aggregated = pd.Series(ohlc_data)
                        else:
                            aggregated = window_df.iloc[-1].copy()
                            
                        aggregated['timestamp'] = window_start
                        window_data.append(aggregated)
                        
                if window_data:
                    symbol_aggregated = pd.DataFrame(window_data)
                    aggregated_data.append(symbol_aggregated)
                    
            if aggregated_data:
                self.processed_data[exchange] = pd.concat(aggregated_data, ignore_index=True)
                logger.info(f"âœ… {exchange}: {len(self.processed_data[exchange])}è¡Œã«é›†ç´„")
            else:
                logger.warning(f"âš ï¸ {exchange}: é›†ç´„å¾Œã«ãƒ‡ãƒ¼ã‚¿ãŒç©ºã«ãªã‚Šã¾ã—ãŸ")
                
        return self.processed_data
        
    def interpolate_missing_data(self, method: str = "forward_fill") -> Dict[str, pd.DataFrame]:
        """æ¬ æãƒ‡ãƒ¼ã‚¿ã®è£œé–“"""
        logger.info(f"ğŸ”§ æ¬ æãƒ‡ãƒ¼ã‚¿è£œé–“ä¸­: {method}æ–¹å¼...")
        
        for exchange, df in self.processed_data.items():
            original_len = len(df)
            
            if method == "forward_fill":
                # å‰å€¤åŸ‹ã‚
                df = df.sort_values(['symbol', 'timestamp'])
                df[['bid', 'ask', 'last']] = df.groupby('symbol')[['bid', 'ask', 'last']].fillna(method='ffill')
                
            elif method == "linear":
                # ç·šå½¢è£œé–“
                df = df.sort_values(['symbol', 'timestamp'])
                for symbol in self.symbols:
                    mask = df['symbol'] == symbol
                    df.loc[mask, ['bid', 'ask', 'last']] = \
                        df.loc[mask, ['bid', 'ask', 'last']].interpolate(method='linear')
                        
            elif method == "spline":
                # ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³è£œé–“
                df = df.sort_values(['symbol', 'timestamp'])
                for symbol in self.symbols:
                    mask = df['symbol'] == symbol
                    symbol_data = df.loc[mask, ['bid', 'ask', 'last']]
                    if len(symbol_data) >= 4:  # ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³è£œé–“ã«ã¯æœ€ä½4ç‚¹å¿…è¦
                        df.loc[mask, ['bid', 'ask', 'last']] = \
                            symbol_data.interpolate(method='spline', order=3)
                            
            # æ®‹ã£ãŸæ¬ æå€¤ã‚’å‰å€¤åŸ‹ã‚ã§å‡¦ç†
            df[['bid', 'ask', 'last']] = df.groupby('symbol')[['bid', 'ask', 'last']].fillna(method='ffill')
            
            self.processed_data[exchange] = df
            
            filled_count = original_len - len(df.dropna(subset=['bid', 'ask']))
            if filled_count > 0:
                logger.info(f"ğŸ”§ {exchange}: {filled_count}ç®‡æ‰€ã®æ¬ æãƒ‡ãƒ¼ã‚¿ã‚’è£œé–“")
                
        return self.processed_data
        
    def create_synchronized_dataset(self) -> pd.DataFrame:
        """å–å¼•æ‰€é–“ã§åŒæœŸã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        logger.info("ğŸ”„ å–å¼•æ‰€é–“ãƒ‡ãƒ¼ã‚¿åŒæœŸä¸­...")
        
        if not self.processed_data:
            raise ValueError("å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            
        synchronized_data = []
        
        # å…±é€šã®æ™‚é–“çª“ã‚’å–å¾—
        all_timestamps = set()
        for df in self.processed_data.values():
            all_timestamps.update(df['timestamp'].dt.floor('30S').unique())
            
        common_timestamps = sorted(all_timestamps)
        
        for timestamp in common_timestamps:
            timestamp_data = {'timestamp': timestamp}
            
            # å„å–å¼•æ‰€ãƒ»ã‚·ãƒ³ãƒœãƒ«ã®ä¾¡æ ¼ã‚’åé›†
            for exchange, df in self.processed_data.items():
                exchange_data = df[df['timestamp'].dt.floor('30S') == timestamp]
                
                for symbol in self.symbols:
                    symbol_data = exchange_data[exchange_data['symbol'] == symbol]
                    
                    if len(symbol_data) > 0:
                        latest = symbol_data.iloc[-1]
                        timestamp_data[f"{exchange}_{symbol}_bid"] = latest['bid']
                        timestamp_data[f"{exchange}_{symbol}_ask"] = latest['ask']
                        if 'last' in latest and pd.notna(latest['last']):
                            timestamp_data[f"{exchange}_{symbol}_last"] = latest['last']
                            
            # ãƒ‡ãƒ¼ã‚¿ãŒååˆ†ã«ã‚ã‚‹æ™‚é–“çª“ã®ã¿è¿½åŠ 
            price_columns = [k for k in timestamp_data.keys() if k.endswith(('_bid', '_ask'))]
            if len(price_columns) >= 8:  # æœ€ä½4ã‚·ãƒ³ãƒœãƒ«Ã—2ä¾¡æ ¼
                synchronized_data.append(timestamp_data)
                
        if synchronized_data:
            sync_df = pd.DataFrame(synchronized_data)
            logger.info(f"âœ… åŒæœŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {len(sync_df)}è¡Œ")
            return sync_df
        else:
            raise ValueError("åŒæœŸå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            
    def save_processed_data(self, output_dir: str, date: str, 
                          save_individual: bool = True, 
                          save_synchronized: bool = True) -> List[Path]:
        """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        saved_files = []
        
        # å€‹åˆ¥å–å¼•æ‰€ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        if save_individual and self.processed_data:
            for exchange, df in self.processed_data.items():
                file_path = output_path / f"{exchange}_processed_{date}.csv"
                df.to_csv(file_path, index=False)
                saved_files.append(file_path)
                logger.info(f"ğŸ’¾ {exchange}ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {file_path}")
                
        # åŒæœŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜
        if save_synchronized:
            try:
                sync_df = self.create_synchronized_dataset()
                sync_path = output_path / f"synchronized_prices_{date}.csv"
                sync_df.to_csv(sync_path, index=False)
                saved_files.append(sync_path)
                logger.info(f"ğŸ’¾ åŒæœŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜: {sync_path}")
            except ValueError as e:
                logger.warning(f"âš ï¸ åŒæœŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå¤±æ•—: {e}")
                
        return saved_files
        
    def generate_preprocessing_report(self) -> str:
        """å‰å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if not self.raw_data and not self.processed_data:
            return "ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“"
            
        report_lines = [
            "=" * 80,
            "ğŸ“Š DATA PREPROCESSING REPORT",
            "=" * 80,
            f"å‡¦ç†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            ""
        ]
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
        if self.raw_data:
            report_lines.extend([
                "ğŸ“‚ ç”Ÿãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:",
                "-" * 40
            ])
            
            total_raw = 0
            for exchange, df in self.raw_data.items():
                total_raw += len(df)
                report_lines.append(f"  {exchange}: {len(df):,}è¡Œ")
                
            report_lines.extend([
                f"  åˆè¨ˆ: {total_raw:,}è¡Œ",
                ""
            ])
            
        # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
        if self.processed_data:
            report_lines.extend([
                "ğŸ”§ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:",
                "-" * 40
            ])
            
            total_processed = 0
            for exchange, df in self.processed_data.items():
                total_processed += len(df)
                # ãƒ‡ãƒ¼ã‚¿æœŸé–“ã‚’è¨ˆç®—
                if len(df) > 0:
                    start_time = df['timestamp'].min()
                    end_time = df['timestamp'].max()
                    duration = (end_time - start_time).total_seconds() / 3600
                    report_lines.extend([
                        f"  {exchange}: {len(df):,}è¡Œ",
                        f"    æœŸé–“: {start_time} ï½ {end_time}",
                        f"    æ™‚é–“: {duration:.1f}æ™‚é–“"
                    ])
                else:
                    report_lines.append(f"  {exchange}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                    
            report_lines.extend([
                f"  åˆè¨ˆ: {total_processed:,}è¡Œ",
                ""
            ])
            
            # ãƒ‡ãƒ¼ã‚¿å“è³ªçµ±è¨ˆ
            report_lines.extend([
                "ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿å“è³ª:",
                "-" * 40
            ])
            
            for exchange, df in self.processed_data.items():
                if len(df) > 0:
                    # å„ã‚·ãƒ³ãƒœãƒ«ã®ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°
                    symbol_counts = df['symbol'].value_counts()
                    missing_data = df[['bid', 'ask']].isna().sum().sum()
                    
                    report_lines.append(f"  {exchange}:")
                    for symbol, count in symbol_counts.items():
                        report_lines.append(f"    {symbol}: {count}ç‚¹")
                    report_lines.append(f"    æ¬ æãƒ‡ãƒ¼ã‚¿: {missing_data}ç®‡æ‰€")
                    
        report_lines.extend([
            "",
            "=" * 80
        ])
        
        return "\n".join(report_lines)


def main():
    parser = argparse.ArgumentParser(description="ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ")
    parser.add_argument("--date", required=True, help="å‡¦ç†ã™ã‚‹æ—¥ä»˜ (ä¾‹: 20250623)")
    parser.add_argument("--data-dir", default="data/price_logs", help="å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--output-dir", default="data/processed", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--window", default="30s", help="æ™‚é–“çª“ã‚µã‚¤ã‚º (ä¾‹: 30s, 1min, 5min)")
    parser.add_argument("--method", default="last", 
                       choices=["last", "mean", "ohlc"], 
                       help="é›†ç´„æ–¹æ³•")
    parser.add_argument("--interpolate", action="store_true", 
                       help="æ¬ æãƒ‡ãƒ¼ã‚¿ã®è£œé–“ã‚’å®Ÿè¡Œ")
    parser.add_argument("--interpolate-method", default="forward_fill",
                       choices=["forward_fill", "linear", "spline"],
                       help="è£œé–“æ–¹æ³•")
    parser.add_argument("--report", help="ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«")
    
    args = parser.parse_args()
    
    try:
        preprocessor = DataPreprocessor(args.data_dir)
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        preprocessor.load_data(args.date)
        
        if not preprocessor.raw_data:
            logger.error("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return 1
            
        # ãƒ‡ãƒ¼ã‚¿é›†ç´„
        preprocessor.aggregate_data(args.window, args.method)
        
        # è£œé–“å‡¦ç†
        if args.interpolate:
            preprocessor.interpolate_missing_data(args.interpolate_method)
            
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        saved_files = preprocessor.save_processed_data(
            args.output_dir, args.date, 
            save_individual=True, 
            save_synchronized=True
        )
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = preprocessor.generate_preprocessing_report()
        
        if args.report:
            with open(args.report, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {args.report}")
        else:
            print(report)
            
        logger.info(f"âœ… å‰å‡¦ç†å®Œäº†: {len(saved_files)}ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜")
        for file_path in saved_files:
            logger.info(f"ğŸ“ {file_path}")
            
        return 0
        
    except Exception as e:
        logger.error(f"âŒ å‰å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return 1


if __name__ == "__main__":
    exit(main())